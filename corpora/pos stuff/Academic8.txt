We compare in this paper several feature selection methods for the Naive Bayes Classifier (NBC) when the data under study are described by a large number of redundant binary indicators. Wrapper approaches guided by the NBC estimation of the classification error probability outperform filter approaches while retaining a reasonable computational cost. 1 Introduction We consider in this paper application contexts in which a large body of expert knowledge is available as a series of simple and low level parametric scores. The goal is to build an interpretable classifier from this knowledge (and from a learning set). Then the scores are assumed to be simple parametric functions from the data space to R with the interpretation that a high value of a score indicates that the datum submitted to the score belongs probably to the class that the score has been designed to detect. Let’s consider a concrete example from our main application domain, aircraft engine monitoring (see [7] for details). We aim here at classifying some short time series (around 150 time points, each series having its own specific length) into different classes (normal signal and different types of anomalies corresponding to some non stationarity in the signal).
