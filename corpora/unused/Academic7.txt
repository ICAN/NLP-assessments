Perceptual similarity is a cognitive judgment that represents the end-stage of a complex cascade of hierarchical processing throughout visual cortex. Although it is intuitive that visual objects that appear similar may share similar underlying patterns of neural activation, a direct mapping between perceptual similarity and representational distance has not been demonstrated. Here we explore the relationship between the human brain’s time-varying representation of visual patterns and behavioral judgments of perceptual similarity. The visual stimuli were abstract patterns constructed from identical perceptual units (oriented Gabor patches) so that each pattern had a unique global form or perceptual 'Gestalt'. The visual stimuli were decodable from evoked neural activation patterns measured with magnetoencephalography (MEG), however, stimuli differed in the similarity of their neural representation as estimated by differences in decodability. Early after stimulus onset (from 50ms), a model based on retinotopic organization predicted the representational similarity of the visual stimuli. Following the peak correlation between the retinotopic model and neural data at 80ms, the neural representations quickly evolved so that retinotopy no longer provided a sufficient account of the brain’s time-varying representation of the stimuli. Overall the strongest predictor of the brain’s representation was a model based on human judgments of perceptual similarity, which reached the theoretical limits of the maximum correlation with the neural data defined by the 'noise ceiling'. Our results show that large-scale brain activation patterns contain a neural signature for the perceptual Gestalt of composite visual features, and demonstrate a strong correspondence between perception and complex patterns of brain activity. Keywords: magnetoencephalography (MEG), representational similarity analysis, perceptual similarity, representational geometry, decoding, Gestalt perception 3 Introduction Judgments of perceptual similarity require integrating information across a complex hierarchical network of brain regions. An early idea of how perceptual similarity might be conceived at the neural level is as a product of representational distance (Shepard, 1964; Torgerson, 1965). Specifically, visual objects that appear similar are assumed to share similar underlying neural representations. Although there has been substantial interest in revealing the transformation of representational structure across the visual processing hierarchy, a direct mapping between perceptual similarity and the similarity of brain activation patterns has not been established. Several studies have observed a correspondence between behavioral similarity judgments and neural representations; however, most of these experiments have focused on object recognition, and thus used stimuli in which perceptual similarity is unavoidably conflated with conceptual similarity (Edelman et al 1998; Mur et al 2013; Connolly et al. 2012). Other studies have emphasized the role of image statistics, and used naturalistic stimuli varying on both semantic and visual dimensions (Hiramatsu et al 2011), in which the mapping between different feature dimensions and perceptual similarity is complex. The aim of the present study is to determine the extent to which perceptual similarity is accessible in dynamic large-scale brain activation patterns. We use two methodological advances to build on previous work: firstly, we use a novel stimulus set to decouple perceived similarity from both semantics and lower-level visual features, and secondly, we exploit the fine temporal resolution of MEG to examine the evolving representational geometry across time. To control for low-level features and semantics in the visual stimuli, we constructed visual patterns from arrangements of Gabor patches, which will drive the response of neurons in early visual cortex, and make straightforward predictions for a range of models that can be used to characterize the evoked cortical response to the stimuli. The stimulus set varied along three dimensions: the number of elements, the local orientation of each Gabor patch, and the degree of orientation coherence among the elements. Critically, although the stimuli are constructed from identical elements, each stimulus has a unique global form or perceptual 'Gestalt' (Figure 1A). 4 Most studies examining representational geometry have used fMRI (e.g. Clarke and Tyler, 2014; Edelman et al., 1998; Hiramatsu et al. 2011; Mur et al., 2013), and focused on the transformation of the representational space across spatial networks of brain regions. Compared to other neuroimaging methods, fMRI has limited temporal resolution, and consequently the temporal evolution of the mapping between behaviorally relevant features and the structure of neural representations has remained largely unexplored. To address this we used MEG to record time-varying patterns of brain activation in response to each visual stimulus. Thus our focus here is on the temporal (rather than spatial) evolution of the neural representational geometry in response to visual patterns.